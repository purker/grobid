<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>Belief Revision: A Critique <lb/>Nir Friedman <lb/>Computer Science Department <lb/>Stanford University <lb/>Gates Building 1A <lb/>Stanford, CA 94305-9010 <lb/>nir@cs.stanford.edu <lb/>Joseph Y. Halpern <lb/>IBM Research Division <lb/>Almaden Research Center, Dept. K53-B2 <lb/>650 Harry Road <lb/>San Jose, CA 95120-6099 <lb/>halpern@almaden.ibm.com <lb/>May 6, 1996 <lb/>Abstract <lb/>The problem of belief change-how an agent should revise her beliefs upon learning new <lb/>information-has been an active area of research in both philosophy and artificial intelligence. <lb/>Many approaches to belief change have been proposed in the literature. Our goal is not to <lb/>introduce yet another approach, but to examine carefully the rationale underlying the approaches <lb/>already taken in the literature, and to highlight what we view as methodological problems in the <lb/>literature. The main message is that to study belief change carefully, we must be quite explicit <lb/>about the &quot;ontology&quot; or scenario underlying the belief change process. This is something that <lb/>has been missing in previous work, with its focus on postulates. Our analysis shows that we <lb/>must pay particular attention to two issues which have often been taken for granted: The first <lb/>is how we model the agent&apos;s epistemic state. (Do we use a set of beliefs, or a richer structure, <lb/>such as an ordering on worlds? And if we use a set of beliefs, in what language are these <lb/>beliefs are expressed?) The second is the status of observations. (Are observations known to <lb/>be true, or just believed? In the latter case, how firm is the belief?) For example, we argue that <lb/>even postulates that have been called &quot;beyond controversy&quot; are unreasonable when the agent&apos;s <lb/>beliefs include beliefs about her own epistemic state as well as the external world. Issues of the <lb/>status of observations arise particularly when we consider iterated belief revision, and we must <lb/>confront the possibility of revising by and then by <lb/>¡ <lb/>. <lb/>Keyword: Belief revision <lb/></front>

			<body>1 Introduction <lb/>The problem of belief change-how an agent should revise her beliefs upon learning new <lb/>information-has been an active area of research in both philosophy and artificial intelligence. <lb/>The problem is a fascinating one in part because it is clearly no unique answer. Nevertheless, <lb/>there is a strong intuition that one wants to make minimal changes, and all the approaches <lb/>to belief change in the literature, such as [AGM85, Gär88, KM91a], try to incorporate this <lb/>principle. However, approaches differ on what constitutes a minimal change. This issue has <lb/>come to the fore with the spate of recent work on iterated belief revision (see, for example, <lb/>[Bou93, BG93, DP94, FL94, Leh95, Lev88, Wil94]). <lb/>The approaches to belief change typically start with a collection of postulates, argue that <lb/>they are reasonable, and prove some consequences of these postulates. Occasionally, a semantic <lb/>model for the postulates is provided and a representation theorem is proved (of the form that <lb/>every semantic model corresponds to some belief revision process, and that every belief revision <lb/>process can be captured by some semantic model). Our goal in this paper is not to introduce <lb/>yet another model of belief change, but to examine carefully the rationale underlying the <lb/>approaches in the literature. The main message of the paper is that describing postulates and <lb/>proving a representation theorem is not enough. While it may have been reasonable when <lb/>research on belief change started in the early 1980s to just consider the implications of a <lb/>number of seemingly reasonable postulates, it is our view that it should no longer be acceptable <lb/>now just to write down postulates and give short English justifications for them. In addition, <lb/>it is important to describe, what, for want of a better word, we call the underlying ontology or <lb/>scenario for the belief change process. Roughly speaking, this means describing carefully what <lb/>it means for something to be believed by an agent and what the status is of new information that <lb/>is received by the agent. This point will hopefully become clearer as we present our critique. <lb/>We remark that even though the issue of ontology is tacitly acknowledged in a number of <lb/>papers (for example, in the last paragraph of [Leh95]), it rarely enters into the discussion in a <lb/>significant way. We hope to show that ontology must play a central role in all discussions of <lb/>belief revision. <lb/>Our focus is on approaches that take as their starting point the postulates for belief revision <lb/>proposed by Alchourrón, Gärdenfors, and Makinson (AGM from now on) [AGM85], but our <lb/>critique certainly applies to other approaches as well. The AGM approach assumes that an <lb/>agent&apos;s epistemic state is represented by a belief set, that is, a set <lb/>¢ <lb/>of formulas in a logical <lb/>language <lb/>£ <lb/>. What the agent learns is assumed to be characterized by some formula <lb/>¤ <lb/>, also in <lb/>£ <lb/>; <lb/>¢ <lb/>¦ ¥ <lb/> § ¤ <lb/>describes the belief set of an agent that starts with belief set <lb/>¢ <lb/>and learns <lb/>¤ <lb/>. <lb/>There are two assumptions implicit in this notation: <lb/>The functional form of <lb/>¥ <lb/>suggests that all that matters regarding how an agent revises her <lb/>beliefs is the belief set and what is learnt. <lb/>The notation suggests that the second argument of <lb/>¥ <lb/>can be an arbitrary formula in <lb/>£ <lb/>. But <lb/>what does it mean to revise by false? In what sense can false be learnt? More generally, is <lb/>it reasonable to assume that an arbitrary formula can be learnt in a given epistemic state? <lb/>The first assumption is particularly problematic when we consider the postulates that AGM <lb/></body>

			<page>1 <lb/></page>

			<body>require <lb/>¥ <lb/>t o satisfy. These essentially state that the agent is consistent in her choices, in the sense <lb/>that she acts as though she has an ordering on the strength of her beliefs [GM88, Gro88], or <lb/>an ordering on possible worlds [Bou94, Gro88, KM91b], or some other predetermined manner <lb/>of choosing among competing beliefs [AGM85]. However, the fact that an agent&apos;s epistemic <lb/>state is characterized by a collection of formulas means that the epistemic state cannot include <lb/>information about relative strength of beliefs (as required for the approach of, say, [GM88]), <lb/>unless this information is expressible in the language. Note that if <lb/>£ <lb/>is propositional logic <lb/>or first-order logic, such information cannot be expressed. On the other hand, if <lb/>£ <lb/> contains <lb/>conditional formulas of the form <lb/>© <lb/>, interpreted as &quot;if <lb/>© <lb/>is learnt, then <lb/>w ill be believed&quot;, <lb/>then such information can be expressed. <lb/>Problems arise when the language is not rich enough to express relative degrees of strength <lb/>in beliefs. Consider, for example, a situation where <lb/>¢ <lb/>! <lb/>&quot; © <lb/>$ # <lb/>% <lb/>&apos; &amp; <lb/>( the logical closure of <lb/>© <lb/>$ # <lb/>% <lb/>; <lb/>that is, the agent&apos;s beliefs are characterized by the formula <lb/>© <lb/>( # <lb/>) <lb/>a nd its logical consequences), <lb/>and then the agent learns <lb/>¤ <lb/>1 0 <lb/>2 © <lb/>4 3 <lb/>5 0 <lb/>6 <lb/>. We can imagine that an agent whose belief in <lb/>© <lb/>is <lb/>stronger than her belief in <lb/>w ould have <lb/>¢ <lb/>1 ¥ <lb/>$ ¤ <lb/>7 <lb/>8 <lb/>9 © <lb/>A @ <lb/>. That is, the agent gives up her belief in <lb/>, but retains a belief in <lb/>© <lb/>. On the other hand, if the agent&apos;s belief in <lb/>i s stronger than her belief <lb/>in <lb/>© <lb/>, it seems reasonable to expect that <lb/>¢ <lb/>B ¥ <lb/>C ¤ <lb/>D <lb/>E 8 <lb/>F <lb/>&apos; @ <lb/>. This suggests that it is unreasonable to <lb/>take <lb/>¥ <lb/>t o be a function if the representation language is not rich enough to express what may be <lb/>significant details of an agent&apos;s epistemic state. <lb/>We could, of course, assume that information about the relative strength of beliefs in various <lb/>propositions is implicit in the choice of the revision operator <lb/>¥ <lb/>, even if it is not contained in the <lb/>language. This is perfectly reasonable, and also makes it more reasonable that <lb/>¥ <lb/>b e a function. <lb/>However, note that we can then no longer assume that we use the same <lb/> ¥ <lb/>when doing iterated <lb/>revision, since there is no reason to believe that the relative strength of beliefs is maintained <lb/>after we learn a formula. In fact, in a number of recent papers [Bou93, BG93, FH95b, Wil94], <lb/>¥ <lb/>is defined as a function from (epistemic states <lb/>G <lb/>formulas) to epistemic states, but the epistemic <lb/>states are no longer just belief sets; they include information regarding relative strengths of <lb/>beliefs. The revision function on epistemic states induces a mapping from (belief sets <lb/>G <lb/>formulas) to belief sets, but at the level of belief sets, the mapping may not be functional; for a <lb/>belief set <lb/>¢ <lb/>and formula <lb/>¤ <lb/>, the belief set <lb/>¢ <lb/>H ¥ <lb/>$ ¤ <lb/>may depend on what epistemic state induced <lb/>¢ <lb/>. Thus, the effect of <lb/>¥ <lb/>o n belief sets may change over time. 1 <lb/>There is certainly no agreement on what postulates belief change should satisfy. However, <lb/>the following two postulates are almost universal: <lb/>¤ <lb/>P I <lb/>Q ¢ <lb/>¦ ¥ <lb/> § ¤¨ <lb/>i f <lb/>¢ <lb/>is consistent and <lb/>¤ <lb/>R I <lb/>¢ <lb/>, then <lb/>¢ <lb/>B ¥ <lb/>$ ¤ <lb/>5 <lb/>S ¢ <lb/>. <lb/>These postulates have been characterized by Rott [Rot89] as being &quot;beyond controversy&quot;. <lb/>Nevertheless, we argue that they are not as innocent as they may at first appear. <lb/>The first postulate says that the agent believes the last thing she learns. Making sense of <lb/>this requires some discussion of the underlying ontology. For example, imagine a scientist <lb/></body>

			<note place="footnote">1 Freund and Lehmann [FL94] have called the viewpoint that <lb/>T <lb/>m ay change over time the dynamic point of view. <lb/>However, this seems somewhat of a misnomer when applied to papers such as [Bou93, BG93, FH95b, Wil94], <lb/>since there <lb/>T <lb/>i n fact is static, when viewed as a function on epistemic states and formulas. <lb/></note>

			<page>2 <lb/></page>

			<body>who believes that heavy objects drop faster than light ones, climbs the tower of Pisa, drops a <lb/>5 kilogram textbook and a 500 milligram novel, and observes they hit the ground at the same <lb/>time. Should the scientist necessarily believe that the time for an object to fall to the ground is <lb/>independent of its weight, on the basis of this one experiment? Certainly when scientists make <lb/>an observation that conflicts with their previous beliefs, they do not immediately change those <lb/>beliefs. This is even more true if they get information (perhaps as a result of reading a paper) <lb/>that is inconsistent with their previous beliefs. One could certainly imagine an ontology where <lb/>it takes repeated observations of <lb/>¤ <lb/>before <lb/>¤ <lb/>is accepted. Roughly speaking, an epistemic state <lb/>would then have to keep track of how many times, and under what circumstances, a proposition <lb/>has been observed. This requires either a rich language or an epistemic state that is described <lb/>by more than just a set of formulas. <lb/>Implicit in Gärdenfors&apos; discussion [Gär88] is a somewhat different assumption: if we decide <lb/>to revise by <lb/>¤ <lb/>, it is because we give <lb/>¤ <lb/>very high epistemic importance. In particular, if <lb/>¢ <lb/>contains <lb/>0 <lb/>6 ¤ <lb/>, we give <lb/>¤ <lb/>higher importance than <lb/>0 <lb/>6 ¤ <lb/>. In the case of our scientist, this means <lb/>that the experiment was repeated, perhaps with some variations, and enough times so as to give <lb/>strong support to the new belief. While this position is again not unreasonable, it seems hard to <lb/>believe that false would ever be given such high epistemic importance. More generally, it is far <lb/>from obvious that in a given epistemic state <lb/>¢ <lb/>we should allow arbitrary consistent formulas <lb/>to be given high epistemic importance. <lb/>If we can actually talk about epistemic importance in the language, then the second postulate <lb/>is no longer so reasonable. For suppose that <lb/>¤ <lb/>D I <lb/>7 ¢ <lb/>. Why should <lb/>¢ <lb/>B ¥ <lb/>C ¤ <lb/>P <lb/>U ¢ <lb/>? It could well <lb/>be that being informed of <lb/>¤ <lb/>raises the importance of <lb/>¤ <lb/>in the epistemic ordering. If epistemic <lb/>ordering can be talked about in the language, then a notion of minimal change should still <lb/>allow epistemic ordering to change, even when something expected is learned. Even if we <lb/>cannot talk about epistemic ordering in the language, this observation has an impact on iterated <lb/>revisions. For example, one assumption made by Lehmann [Leh95] (his postulate I4) is that if <lb/>© <lb/>is believed after revising by <lb/>¤ <lb/>, then revising by <lb/>V <lb/>W ¤ <lb/>X <lb/>Y © <lb/>X <lb/>b a <lb/>d c <lb/>-that is, revising by <lb/>¤ <lb/>then <lb/>© <lb/>then <lb/>a <lb/>-is equivalent to revising by <lb/>V <lb/>W ¤ <lb/>e X <lb/>f a <lb/>d c <lb/>. But consider a situation where after revising by <lb/>¤ <lb/> , the <lb/>agent believes both <lb/>© <lb/>and <lb/>, but her belief in <lb/>is stronger than her belief in <lb/>© <lb/>. We can well <lb/>imagine that after learning <lb/>0 <lb/>2 © <lb/>g 3 <lb/>h 0 <lb/>6 <lb/>in this situation, she would believe <lb/>0 <lb/>i © <lb/>and <lb/>. However, <lb/>if she first learned <lb/>© <lb/>and then <lb/>0 <lb/>2 © <lb/>4 3 <lb/>7 0 <lb/>6 <lb/>, she would believe <lb/>© <lb/>and <lb/>0 <lb/>6 <lb/>, because, as a result of <lb/>learning <lb/>© <lb/>, she would give <lb/>© <lb/>higher epistemic importance than <lb/>. In this case, we would not <lb/>have <lb/> V <lb/>W ¤ <lb/>h X <lb/>b © <lb/>A <lb/>p 0 <lb/>i © <lb/>4 3 <lb/>h 0 <lb/>6 <lb/>&apos; &amp; <lb/>q c <lb/>d <lb/>H V <lb/>r ¤ <lb/>7 X <lb/>&apos; <lb/>p 0 <lb/>i © <lb/>4 3 <lb/>h 0 <lb/>6 <lb/>&apos; &amp; <lb/>q c <lb/>. In light of this discussion, it is not surprising that <lb/>the combination of the second postulate with a language that can talk about epistemic ordering <lb/>leads to technical problems such as Gärdenfors&apos; triviality result [Gär88]. <lb/>To give a sense of our concerns here, we discuss two basic ontologies. The first ontology <lb/>that seems (to us) reasonable assumes that the agent has some knowledge as well as beliefs. <lb/>We can think of the formulas that the agent knows as having the highest state of epistemic <lb/>importance. In keeping with the standard interpretation of knowledge, we also assume that the <lb/>formulas that the agent knows are true in the world. Since agents typically do not have certain <lb/>knowledge of very many facts, we assume that the knowledge is augmented by beliefs (which <lb/>can be thought of as defeasible guides to action). Thus, the set of formulas that are known form <lb/></body>

			<page>3 <lb/></page>

			<body>a subset of the belief set. We assume that the agent observes the world using reliable sensors; <lb/>thus, if the agent observes <lb/>¤ <lb/>, then the agent is assumed to know <lb/>¤ <lb/>. After observing <lb/>¤ <lb/>, the agent <lb/>adds <lb/>¤ <lb/>to his stock of knowledge, and may revise his belief set. Since the agent&apos;s observations <lb/>are taken to be knowledge, the agent will believe <lb/>¤ <lb/>after observing <lb/>¤ <lb/>. However, the agent&apos;s <lb/>epistemic state may change even if she observes a formula that she previously believed to be <lb/>true. In particular, if the formula observed was believed to be true but not known to be true, <lb/>after the observation it is known. Note that, in this ontology, the agent never observes false, <lb/>since false is not true of the world. In fact, the agent never observes anything that contradicts <lb/>her knowledge. Thus, <lb/>¢ <lb/>s ¥ <lb/>t ¤ <lb/>is defined only for formulas <lb/>¤ <lb/>that are compatible with the agent&apos;s <lb/>knowledge. Moving to iterated revision, this means we cannot have a revision by <lb/>¤ <lb/>followed <lb/>by a revision by <lb/>0 <lb/>6 ¤ <lb/>. This ontology underlies some of our earlier work [FH95a, FH95b]. As <lb/>we show here, a variant of Darwiche and Pearl&apos;s approach [DP94] captures them as well. <lb/>We can consider a second ontology that has a different flavor. In this ontology, if we <lb/>observe something, we believe it to be true and perhaps even assign it a strength of belief. <lb/>But this assignment does not represent the strength of belief of the observation in the resulting <lb/>epistemic state. Rather, the belief in the observation must &quot;compete&quot; against current beliefs <lb/>if it is inconsistent with these beliefs. In this ontology, it is not necessarily the case that <lb/>¤ <lb/>D I <lb/>h ¢ <lb/>B ¥ <lb/> § ¤ <lb/>, just as it is not the case that a scientist will necessarily adopt the consequences of <lb/>his most recent observation into his stock of beliefs (at least, not without doing some additional <lb/>experimentation). Of course, to flesh out this ontology, we need to describe how to combine <lb/>a given strength of belief in the observation with the strengths of the beliefs in the original <lb/>epistemic state. Perhaps the closest parallel in the literature is something like the Dempster-<lb/>Shafer rule of combination [Sha76], which gives a rule for combining two separate bodies of <lb/>belief. We do not have a particular suggestion to make along these lines. However, we believe <lb/>that this type of ontology deserves further study. <lb/>The rest of the paper is organized as follows. In Section 2, we review the AGM framework, <lb/>and point out some problems with it. In Section 3, we consider proposals for belief change and <lb/>iterated belief change from the literature due to Boutilier [Bou93], Darwiche and Pearl [DP94], <lb/>Freund and Lehmann [FL94], and Lehmann [Leh95], and try to understand the ontology implicit <lb/>in the proposal (to the extent that one can be discerned). In Section 4, we consider the first <lb/>ontology discussed above in more detail. We conclude with some discussion in Section 5. <lb/>2 AGM Belief Revision <lb/>In this section we review the AGM approach to belief revision. As we said earlier, this approach <lb/>assumes that beliefs and observations are expressed in some language <lb/>£ <lb/>. It is assumed that <lb/>£ <lb/>is <lb/>closed under negation and conjunction, and comes equipped with a consequence relation <lb/>u <lb/>w v <lb/>t hat <lb/>contains the propositional calculus and satisfies the deduction theorem. The agent&apos;s epistemic <lb/>state is represented by a belief set, that is, a set of formulas in <lb/>£ <lb/>closed under deduction. There <lb/>is also assumed to be a revision operator <lb/>¥ <lb/>t hat takes a belief set <lb/>¢ <lb/>and a formula <lb/>¤ <lb/>and returns <lb/>a new belief set <lb/>¢ <lb/>U ¥ <lb/>x ¤ <lb/>, intuitively, the result of revising <lb/>¢ <lb/>by <lb/>¤ <lb/>. The following AGM postulates <lb/>are an attempt to characterize the intuition of &quot;minimal change&quot;: <lb/></body>

			<page>4 <lb/></page>

			<body>R1. <lb/>¢ <lb/>¦ ¥ <lb/> § ¤ <lb/>is a belief set <lb/>R2. <lb/>¤ <lb/>P I <lb/>Q ¢ <lb/>¦ ¥ <lb/> § ¤ <lb/>R3. <lb/>¢ <lb/>¦ ¥ <lb/> § ¤ <lb/>R y <lb/>! <lb/>¢ <lb/>8 <lb/>F ¤ <lb/>@ <lb/>&amp; <lb/>R4. If <lb/>0 <lb/>6 ¤ <lb/>I <lb/>¢ <lb/>then <lb/>! <lb/>¢ <lb/>8 <lb/>F ¤ <lb/>@ <lb/>&amp; <lb/>y <lb/>¢ <lb/>¦ ¥ <lb/> § ¤ <lb/>R5. <lb/>¢ <lb/>¦ ¥ <lb/> § ¤ <lb/>7 <lb/>% <lb/>false&amp; if and only if <lb/>u <lb/>w <lb/>0 <lb/>6 ¤ <lb/>R6. If <lb/>u <lb/>¤ <lb/>a <lb/>then <lb/>¢ <lb/>1 ¥ <lb/>$ ¤ <lb/>7 <lb/>¢ <lb/>1 ¥ <lb/>a <lb/>R7. <lb/>¢ <lb/>¦ ¥ <lb/>C <lb/>¤ <lb/># <lb/>) a <lb/>&amp; <lb/>y <lb/>! <lb/>¢ <lb/>¥ <lb/> § ¤ <lb/>8 <lb/>a <lb/>C @ <lb/>&amp; <lb/>R8. If <lb/>0 <lb/>t a <lb/>I <lb/>Q ¢ <lb/>¦ ¥ <lb/> § ¤ <lb/>then <lb/>! <lb/>¢ <lb/>¥ <lb/> § ¤ <lb/>Q <lb/>e 8 <lb/>a <lb/>@ <lb/>&amp; <lb/>y <lb/>¢ <lb/>1 ¥ <lb/>¤ <lb/># <lb/>4 a <lb/>&amp; <lb/>There are several representation theorems for AGM belief revision; perhaps the clearest is <lb/>due to Grove [Gro88]. We discuss a slight modification, due to Boutilier [Bou94] and Katsuno <lb/>and Mendelzon [KM91b]: Let an <lb/>£ <lb/>-world be a complete and consistent truth assignment to <lb/>the formulas in <lb/>£ <lb/>. Let <lb/>consist of all the <lb/>£ <lb/>-worlds, and let <lb/>be a ranking, that is, a total <lb/>preorder, on the worlds in <lb/>. Let mind consist of all the minimal worlds with respect to <lb/>, <lb/>that is, all the worlds <lb/>e <lb/>such that there is no <lb/>e <lb/>f <lb/>w ith <lb/>e <lb/>C f <lb/>$ g <lb/>1 e <lb/>. With <lb/>we can associate a <lb/>belief set <lb/>¢ <lb/>h d <lb/>, consisting of all formulas <lb/>¤ <lb/>that are true in all the worlds in mind . Moreover, <lb/>we can define a revision operator <lb/>¥ <lb/>on <lb/>¢ <lb/>d <lb/>, by taking <lb/>¢ <lb/>d <lb/>¥ <lb/>i ¤ <lb/>to consist of all formulas <lb/>a <lb/>that <lb/>are true in all the minimal <lb/>¤ <lb/>-worlds according to <lb/>. It can be shown that <lb/>¥ <lb/>satisfies the AGM <lb/>postulates (when its first argument is <lb/>¢ <lb/>d <lb/>) . Thus, we can define a revision operator by taking a <lb/>collection of orderings <lb/>C j <lb/>, one for each belief set <lb/>¢ <lb/>. To define <lb/>¢ <lb/>B ¥ <lb/>C ¤ <lb/>for a belief set <lb/>¢ <lb/>, we <lb/>apply the procedure above, starting with the ranking <lb/>j <lb/>corresponding to <lb/>¢ <lb/>. 2 Furthermore, <lb/>in [Bou94, Gro88, KM91b], it is shown that every belief revision operator satisfying the AGM <lb/>axioms can be characterized in this way. <lb/>This elegant representation theorem also brings out some of the problems with the AGM <lb/>postulates. First, note that a given revision operator <lb/>¥ <lb/>is represented by a family of rankings, <lb/>one for each belief set. There is no necessary connection between the rankings corresponding <lb/>to different belief sets. It might seem more reasonable to have a more global setting (perhaps <lb/>one global ranking) from which each element in the family of rankings arises. <lb/>A second important point is that the epistemic state here is represented not by a belief set, <lb/>but by a ranking. Each ranking <lb/>is associated with a belief set <lb/>¢ <lb/>d <lb/>, but it is the ranking that <lb/>gives the information required to describe how revision is carried out. The belief set does not <lb/>suffice to determine the revision; there are many rankings <lb/>for which the associated belief set <lb/>¢ <lb/>d <lb/>is <lb/>¢ <lb/>. Since the revision process only gives us the revised belief set, not the revised ranking, <lb/>the representation does not support iterated revision. <lb/>This suggests that we should consider, not how to revise belief sets, but how to revise <lb/>rankings. More generally, whatever we take to be our representation of the epistemic state, it <lb/>seems appropriate to consider how these representations should be revised. This suggests that <lb/>we consider an analogue of the AGM postulates for epistemic states. Such an analogue can be <lb/></body>

			<note place="footnote">2 In this construction, for each belief set <lb/>k <lb/>other than the inconsistent belief set, we have <lb/>k <lb/>l <lb/>n m <lb/>o <lb/>R k <lb/>. The <lb/>inconsistent belief set gets special treatment here. <lb/></note>

			<page>5 <lb/></page>

			<body>defined in a straightforward way (cf. [FH95b]): Taking <lb/>p <lb/>to range over epistemic states and <lb/>Bel <lb/>p <lb/>q &amp; <lb/>t o represent the belief set associated with epistemic state <lb/>p <lb/>, we have <lb/>R1f <lb/>&quot; r <lb/>p <lb/>¥ <lb/>$ ¤ <lb/>is an epistemic state <lb/>R2f <lb/>r <lb/>¤ <lb/>R I <lb/>Bel <lb/>p <lb/>¥ <lb/> § ¤ <lb/>&amp; <lb/>R3f <lb/>&quot; r <lb/>p <lb/>¥ <lb/>$ ¤ <lb/>7 y <lb/>! <lb/>Bel <lb/>p <lb/>q &amp; <lb/>s <lb/>8 <lb/>F ¤ <lb/>@ <lb/>&amp; <lb/>and so on, with the obvious syntactic transformation. In fact, as we shall see in the next section, <lb/>a number of processes for revising epistemic states have been considered in the literature, and <lb/>in fact they all do satisfy these modified postulates. <lb/>Finally, even if we restrict attention to belief sets, we can consider what happens if the <lb/>underlying language <lb/>£ <lb/>is rich enough to talk about how revision should be carried out. For <lb/>example, suppose <lb/>£ <lb/>conditional formulas, and we want to find some ranking <lb/>for which the <lb/>corresponding belief set is <lb/>¢ <lb/>. Not just any ranking <lb/>such that <lb/>¢ <lb/>d <lb/>t ¢ <lb/>will do here. The <lb/>beliefs in <lb/>¢ <lb/>put some constraints on the ranking. For example, if <lb/> © <lb/>7 <lb/>E <lb/>is in <lb/>¢ <lb/>and <lb/>© <lb/>u I <lb/>R ¢ <lb/>, <lb/>then the minimal <lb/>-worlds satisfying <lb/>© <lb/>must all satisfy <lb/>, since after <lb/>© <lb/>is learnt, <lb/>is believed. <lb/>Once we restrict to rankings that are consistent with the formulas in the worlds that are being <lb/>ranked, then the AGM postulates are no longer sound. This point has essentially been made <lb/>before [Bou92, Rot89]. However, it is worth stressing the sensitivity of the AGM postulates to <lb/>the underlying language and, more generally, to the choice of epistemic state. <lb/> 3 Proposals for Iterated Revision <lb/>We now briefly review some of the previous proposals for iterated belief change, and point out <lb/>how the impact of the observations we have been making on the approaches. Most of these <lb/>approaches start with the AGM postulates, and augment them to get seemingly appropriate <lb/>restrictions on iterated revision. This is not an exhaustive review of the literature on iterated <lb/>belief revision by any stretch of the imagination. Rather, we have chosen a few representative <lb/>approaches that allow us to bring out our methodological concerns. <lb/>3.1 Boutilier&apos;s natural revision <lb/>As we said in the previous section, Boutilier takes the agent&apos;s epistemic state to consist of a <lb/>ranking of possible worlds. Boutilier [Bou93] describes a particular revision operator <lb/>¥ <lb/>v <lb/>on <lb/>epistemic states that he calls natural revision operator. Natural revision maps a ranking <lb/>of <lb/>possible worlds and an observation <lb/>¤ <lb/>to a revised ranking <lb/>¥ <lb/>v <lb/>w ¤ <lb/>such that (a) <lb/>¥ <lb/>v <lb/>w ¤ <lb/>satisfies <lb/>the conditions of the representation theorem described above-the minimal worlds in <lb/>¥v <lb/>¤ <lb/>are precisely the minimal <lb/>¤ <lb/>-worlds in <lb/>, and (b) in a precise sense, <lb/>¦ ¥v <lb/>¤ <lb/>is the result of <lb/>making the minimal number of changes to <lb/>required to guarantee that all the minimal worlds <lb/>in <lb/>E ¥v <lb/>¤ <lb/>satisfy <lb/>¤ <lb/>. Given a ranking <lb/>and a formula <lb/>¤ <lb/>, the ranking <lb/>s ¥v <lb/>¤ <lb/>is identical to <lb/>except that the minimal <lb/>¤ <lb/>-worlds according to <lb/>have the minimal rank in the revised ranking, <lb/>while the relative ranks of all other worlds remains unchanged. <lb/>Boutilier characterizes the properties of natural revision. Suppose that, starting in some <lb/>epistemic state, we revise by <lb/>¤ <lb/>1 <lb/>x <lb/>r <lb/>b r <lb/>y r <lb/>x <lb/>¤ <lb/>s z <lb/>. Further suppose <lb/>¤ <lb/>| { <lb/>} <lb/></body>

			<note place="footnote">1 is consistent with the beliefs <lb/></note>

			<page>6 <lb/></page>

			<body>after revising by <lb/>¤ <lb/>1 <lb/>x <lb/>r <lb/>b r <lb/>b r <lb/>x <lb/>¤ <lb/>| { <lb/>. Then the beliefs after revising by <lb/>¤ <lb/>1 <lb/>x <lb/>r <lb/>b r <lb/>b r <lb/>x <lb/>¤ <lb/>| z <lb/>are precisely the <lb/>beliefs after observing <lb/>¤ <lb/>1 <lb/># <lb/>r <lb/>b r <lb/>b r# <lb/>7 ¤ <lb/>s z <lb/>. (More precisely, given any ranking <lb/>, the belief set <lb/>associated with the ranking <lb/>¥ <lb/>v <lb/>w ¤ <lb/>1 <lb/>¥ <lb/>v <lb/>r <lb/>y r <lb/>b r¥ <lb/>v <lb/>¤ <lb/>| z <lb/>is the same as that associated with the <lb/>ranking <lb/>¥ <lb/>v <lb/>¤ <lb/>1 <lb/># <lb/>r <lb/>b r <lb/>b r# <lb/>¤ <lb/>| z <lb/>&amp; <lb/>. Note, however, that <lb/>¥ <lb/>v <lb/>w ¤ <lb/>1 <lb/>¥ <lb/>v <lb/>r <lb/>b r <lb/>b r¥ <lb/>v <lb/>¤ <lb/>| z <lb/>e <lb/>% <lb/>¥ <lb/>v <lb/>( <lb/>¤ <lb/>g # <lb/>r <lb/>b r <lb/>b r# <lb/>¤ <lb/>| z <lb/>&amp; <lb/>in general.) Thus, as long as the agent&apos;s new observations are not surprising, the agent&apos;s <lb/>beliefs are exactly the ones she would have had had she observed the conjunction of all the <lb/>observations. This is an immediate consequence of the AGM postulates, and thus holds for any <lb/>approach that attempts to extend the AGM postulates to iterated revision. <lb/>What happens when the agent observes a formula <lb/>¤ <lb/>| z <lb/>} <lb/>1 that is inconsistent with her current <lb/>beliefs? Boutilier shows that in this case the new observation nullifies the impact of the all the <lb/>observations starting with the most recent one that is inconsistent with <lb/>¤ <lb/>s z <lb/>} <lb/>1 . More precisely, <lb/>suppose <lb/>¤{ <lb/>} <lb/>1 is consistent with the belief after observing <lb/>¤ <lb/>1 <lb/>x <lb/>r <lb/>b r <lb/>y r <lb/>x <lb/>¤{ <lb/>f or <lb/>, but <lb/>¤z <lb/>} <lb/>1 is <lb/>inconsistent with the beliefs after observing <lb/>¤ <lb/>1 <lb/>x <lb/>r <lb/>b r <lb/>b r <lb/>x <lb/>¤z <lb/>. Let <lb/>be the maximal index such that <lb/>¤z <lb/>} <lb/>1 is consistent with the beliefs after learning <lb/>¤ <lb/>1 <lb/>x <lb/>r <lb/>y r <lb/>b r <lb/>x <lb/>¤ <lb/>. The agent&apos;s beliefs after observing <lb/>¤z <lb/>} <lb/>1 are the same as her beliefs after observing <lb/>¤ <lb/>1 <lb/>x <lb/>r <lb/>y r <lb/>b r <lb/>x <lb/>¤ <lb/>x <lb/>¤z <lb/>} <lb/>1 . Thus, the agent acts as <lb/>though she did not observe <lb/>¤ <lb/>t } <lb/>1 <lb/>x <lb/>r <lb/>b r <lb/>b r <lb/>x <lb/>¤z <lb/>. <lb/>Boutilier does not provide any argument for the reasonableness of this ontology. In fact, <lb/>Boutilier&apos;s presentation (like almost all others in the literature) is not in terms of an ontology <lb/>at all; he presents natural revision as an attempt to minimize changes to the ranking. While <lb/>the intuition of minimizing changes to the ranking seems reasonable at first, it becomes less <lb/>reasonable when we realize its ontological implications. The following example, due to <lb/>Darwiche and Pearl [DP94], emphasizes this point. Suppose we encounter a strange new <lb/>animal and it appears to be a bird, so we believe it is a bird. On closer inspection, we see that <lb/>it is red, so we believe that it is a red bird. However, an expert then informs us that it is not a <lb/>bird, but a mammal. Applying natural revision, we would no longer believe that the animal is <lb/>red. This does not seem so reasonable. <lb/>One more point is worth observing: As described by Boutilier [Bou93], natural revision <lb/>does not allow revision by false. While we could, of course, modify the definition to handle <lb/>false, it is more natural simply to disallow it. This suggests that, whatever ontology is used to <lb/>justify natural revision, in that ontology, revising by false should not make sense. <lb/>3.2 Freund and Lehmann&apos;s approach <lb/>Freund and Lehmann [FL94] stick close to the original AGM approach. They work with belief <lb/>sets, not more general epistemic states. However, they are interested in iterated revision. They <lb/>consider the effect of adding just one more postulate to the basic AGM postulates, namely <lb/>R9. If <lb/>0 <lb/>6 ¤ <lb/>P I <lb/>¢ <lb/>, then <lb/>¢ <lb/>¦ ¥ <lb/> § ¤ <lb/>7 <lb/>¢ <lb/>h ¥ <lb/> § ¤ <lb/>, <lb/>where <lb/>¢ <lb/>is the inconsistent belief set, which consists of all formulas. <lb/>Suppose <lb/>¥ <lb/>satisfies K1-K9. Just as with Boutilier&apos;s natural revision, if <lb/>¤{ <lb/>} <lb/>1 is consistent <lb/>with the beliefs after learning <lb/>¤ <lb/>1 <lb/>x <lb/>r <lb/>b r <lb/>y r <lb/>x <lb/>¤ <lb/>| { <lb/>f or <lb/>C <lb/>s <lb/>e <lb/>1, then <lb/>¢ <lb/>¥ <lb/>C ¤ <lb/>1 <lb/>¥ <lb/>r <lb/>b r <lb/>y r¥ <lb/>C ¤ <lb/>| z <lb/>e <lb/>E ¢ <lb/>¥ <lb/>w <lb/>¤ <lb/>1 <lb/># <lb/>r <lb/>b r <lb/>b r# <lb/>Q ¤ <lb/>| z <lb/>&amp; <lb/>. However, if we then observe <lb/>¤ <lb/>s z <lb/>} <lb/>1 , and it is inconsistent with <lb/>¢ <lb/>¥ <lb/>C ¤ <lb/>1 <lb/># <lb/>r <lb/>b r <lb/>b r# <lb/>¤ <lb/>| z <lb/>, <lb/>then <lb/>¢ <lb/>¥ <lb/>d ¤ <lb/>1 <lb/>¥ <lb/>r <lb/>y r <lb/>b r¥ <lb/>d ¤ <lb/>| z <lb/>} <lb/>1 <lb/>¢ <lb/>g ¥ <lb/>d ¤ <lb/>| z <lb/>} <lb/>1 . That is, observing something inconsistent causes us to <lb/></body>

			<page>7 <lb/></page>

			<body>retain none of our previous beliefs, but to start over from scratch. While the ontology here is <lb/>quite simple to explain, as Freund and Lehmann themselves admit, it is a rather severe form of <lb/>belief revision. Darwiche and Pearl&apos;s red bird example applies to this approach as well. <lb/>3.3 Darwiche and Pearl&apos;s approach <lb/>Darwiche and Pearl [DP94] suggest a set of postulates extending the AGM postulates, and <lb/>claim to provide a semantics that satisfies them. Their intuition is that the revision operator <lb/>should retain as must as possible certain parts of the ordering among worlds in the ranking. In <lb/>particular, if <lb/>e <lb/>1 and <lb/>e <lb/>2 both satisfy <lb/>¤ <lb/>, then a revision by <lb/>¤ <lb/>should not change the relative rank <lb/>of <lb/>e <lb/>1 and <lb/>e <lb/>2 . Similarly, if both <lb/>e <lb/>1 and <lb/>e <lb/>2 satisfy <lb/>0 <lb/>6 ¤ <lb/>, then a revision should not change their <lb/>relative rank. They describe four postulates that are meant to embody these intuitions: <lb/>C1. If <lb/>¤ <lb/>u <lb/>) a <lb/>, then <lb/>¢ <lb/>1 ¥ <lb/>d a <lb/>&amp; <lb/>s ¥ <lb/> § ¤ <lb/>5 <lb/>¢ <lb/>¦ ¥ <lb/> § ¤ <lb/>C2. If <lb/>¤ <lb/>u <lb/>e 0 <lb/>t a <lb/>, then <lb/>¢ <lb/>1 ¥ <lb/>a <lb/>C &amp; <lb/>s ¥ <lb/>$ ¤ <lb/>7 <lb/>¢ <lb/>B ¥ <lb/>$ ¤ <lb/>C3. If <lb/>a <lb/>I <lb/>¢ <lb/>¦ ¥ <lb/> § ¤ <lb/>, then <lb/>a <lb/>I <lb/>5 <lb/>¢ <lb/>¦ ¥ <lb/>d a <lb/>&amp; <lb/>s ¥ <lb/> § ¤ <lb/>C4. If <lb/>0 <lb/>t a <lb/>u I <lb/>Q ¢ <lb/>¦ ¥ <lb/> § ¤ <lb/>, then <lb/>0 <lb/>t a <lb/>u I <lb/>h <lb/>¢ <lb/>1 ¥ <lb/>a <lb/>C &amp; <lb/>i ¥ <lb/> § ¤ <lb/>Freund and Lehmann [FL94] point out that C2 is inconsistent with the AGM postulates. <lb/>This observation seems inconsistent with the fact that Darwiche and Pearl claim to provide a <lb/>semantics for their postulates. What is going on here? It turns out that the issues raised earlier <lb/>help clarify the situation. <lb/>Darwiche and Pearl semantics is based on a special case Spohn&apos;s ordinal conditional <lb/>functions (OCFs) [Spo88] called <lb/>-rankings [GP92]. A <lb/>-ranking associates with each world <lb/>either a natural number <lb/>or <lb/>, with the requirement that for at least one world <lb/>e <lb/>0 , we have <lb/>w <lb/>q e <lb/>0 <lb/>&amp; <lb/>0. We can think of <lb/>A <lb/>e <lb/>&amp; <lb/>a s the rank of <lb/>e <lb/>, or as denoting how surprising it would be to <lb/>discover that <lb/>e <lb/>is the actual world. If <lb/>A <lb/>e <lb/>&amp; <lb/>0, then world <lb/>e <lb/>is unsurprising; if <lb/>w <lb/>e <lb/>&amp; <lb/>1, then <lb/>e <lb/>is somewhat surprising; if <lb/>w <lb/>q e <lb/>( &amp; <lb/>2, then <lb/>e <lb/>is more surprising, and so on. If <lb/>w <lb/>e <lb/>&amp; <lb/>C <lb/>, <lb/>then <lb/>e <lb/>is impossible. 3 OCFs provide a way of ranking worlds that is closely related to, but has <lb/>a little more structure than, the orderings considered by Boutilier. The extra structure makes it <lb/>easier to define a notion of conditioning. <lb/>Given a formula <lb/>¤ <lb/>, let <lb/>w <lb/>¤ <lb/>&amp; <lb/>min8 <lb/>A <lb/>e <lb/>&amp; <lb/>: <lb/>e <lb/>¤ <lb/>@ <lb/>; we define <lb/>w <lb/>false&amp; <lb/>. We say that <lb/>¤ <lb/>is believed with firmness <lb/>R <lb/>0 in OCF <lb/>if <lb/>w <lb/>¤ <lb/>&amp; <lb/>d <lb/>0 and <lb/>w <lb/>0 <lb/>6 ¤ <lb/>&amp; <lb/>d <lb/>. Thus, <lb/>¤ <lb/>is believed <lb/>with firmness <lb/>if <lb/>¤ <lb/>is unsurprising and the least surprising world satisfying <lb/>0 <lb/>6 ¤ <lb/>has rank <lb/>. By <lb/>analogy to the definition of <lb/>¢ <lb/>d <lb/>, we define <lb/>¢ <lb/>t o consist of all those formulas that are believed <lb/>with firmness at least 1. <lb/>Spohn defined a notion of conditioning on OCFs. Given an OCF <lb/>, a formula <lb/>¤ <lb/>such that <lb/>w <lb/>¤ <lb/>&amp; <lb/>C <lb/>E <lb/>, and <lb/>D <lb/>0, <lb/>n ¡ <lb/>¢ <lb/>is the unique OCF satisfying the properties desired by Darwiche <lb/>and Pearl-namely, if <lb/>e <lb/>and <lb/>e <lb/>f <lb/>b oth satisfy <lb/>¤ <lb/>or both satisfy <lb/>0 <lb/>6 ¤ <lb/>, then <lb/>£ ¡ <lb/>¢ <lb/>¤ <lb/>e <lb/>&amp; <lb/>6 <lb/>D <lb/>£ ¡ <lb/>¢ <lb/>¥ <lb/>q e <lb/>f&amp; <lb/></body>

			<note place="footnote">3 Spohn allowed ranks to be arbitrary ordinals, not just natural numbers, and did not allow a rank of <lb/>¦ <lb/>, since, <lb/>for philosophical reasons, he did not want to allow a world to be considered impossible. As we shall see, there <lb/>are technical advantages to introducing a rank of <lb/>¦ <lb/>. <lb/></note>

			<page>8 <lb/></page>

			<body>w <lb/>q e <lb/>( &amp; <lb/>t <lb/>D <lb/>w <lb/>q e <lb/>f&amp; <lb/>-such that <lb/>¤ <lb/>is believed with firmness <lb/>in <lb/>£ ¡ <lb/>¢ <lb/>. It is defined as follows: <lb/>£ ¡ <lb/>¢ <lb/>¤ <lb/>e <lb/>&amp; <lb/> § <lb/>A <lb/>e <lb/>&amp; <lb/>t <lb/>D <lb/>w <lb/>¤ <lb/>&amp; <lb/>if <lb/>e <lb/>satisfies <lb/>¤ <lb/>A <lb/>e <lb/>&amp; <lb/>t <lb/>D <lb/>w <lb/>p 0 <lb/>¤ <lb/>&amp; <lb/>s <lb/>R <lb/>if <lb/>e <lb/>satisfies <lb/>0 <lb/>6 ¤ <lb/>. <lb/>Notice that <lb/>£ ¡ <lb/>¢ <lb/>is defined only if <lb/>w <lb/>¤ <lb/>&amp; <lb/>d <lb/>, that is, if <lb/>¤ <lb/>is considered possible. <lb/>Darwiche and Pearl defined the following revision function on OCFs: <lb/>% ¥ <lb/>© <lb/>s ª <lb/>7 ¤ <lb/>7 <lb/> § <lb/>if <lb/>¤ <lb/>is believed with firmness <lb/>7 <lb/>1 in <lb/>¥ <lb/>£ ¡ <lb/>1 otherwise. <lb/>Thus, if <lb/>¤ <lb/>is already believed with firmness at least 1 in <lb/>, then <lb/>is unaffected by a revision <lb/>by <lb/>¤ <lb/>; otherwise, the effect of revision is to modify <lb/>by conditioning so that <lb/>¤ <lb/>ends up being <lb/>believed with degree of firmness 1. Intuitively, this means that if <lb/>¤ <lb/>is not believed in <lb/>, in <lb/>C ¥ <lb/>6 ¤ <lb/>it is believed, but with the minimal degree of firmness. <lb/>It is not hard to show that if we take an agent&apos;s epistemic state to be represented by an <lb/>OCF, then Darwiche and Pearl&apos;s semantics satisfies all the AGM postulates modified to apply <lb/>to epistemic states (that is, R1f -R8f in Section 2), except that revising by false is disallowed, <lb/>just as in natural revision, so that R5f holds vacuously; in addition, this semantics satisfies <lb/>Darwiche and Pearl&apos;s C1-C4, modified to apply to epistemic states. For example, C2 becomes <lb/>C2f <lb/>&quot; r <lb/>I f <lb/>¤ <lb/>u <lb/>e 0 <lb/>a <lb/>, then <lb/>¢ <lb/>g « <lb/>9 ¬ <lb/>&apos; ® <lb/>¬ <lb/>¢ <lb/>9 ¬ <lb/>° <lb/>. <lb/>Indeed, as Darwiche and Pearl observe, natural revision also satisfies C1f -C4f , however, it has <lb/>properties that they view as undesirable. Thus, Darwiche and Pearl&apos;s claim that their postulates <lb/>are consistent with AGM is correct, if we think at the level of general epistemic states. On <lb/>the other hand, Freund and Lehmann are quite right that R1-R8 and C1-C4 are incompatible; <lb/>indeed, as they point out, R1-R4 and C2 are incompatible. The importance of making clear <lb/>exactly whether we are considering the postulates with respect to the OCF <lb/>or the belief set <lb/>¢ <lb/>i s particularly apparent here. <lb/>The fact that Boutilier&apos;s natural revision also satisfies C1f -C4f clearly shows that these <lb/>postulates do not capture all of Darwiche and Pearl&apos;s intuitions. Their semantics embodies <lb/>further assumptions. Some of them seem ad hoc. Why is it reasonable to believe <lb/>¤ <lb/>with <lb/>a minimal degree of firmness after revising by <lb/>¤ <lb/>? Rather than trying to come up with an <lb/>improved collection of postulates (which Darwiche and Pearl themselves suggest might be a <lb/>difficult task), it seems to us a more promising approach is to find an appropriate ontology. <lb/>3.4 Lehmann&apos;s revised approach <lb/>Finally, we consider Lehmann&apos;s &quot;revised&quot; approach to belief revision [Leh95]. With each <lb/>sequence <lb/>± <lb/>of observations, Lehmann associates a belief set that we denote Bel <lb/>± <lb/>&amp; <lb/>. Intuitively, <lb/>we can think of Bel <lb/>± <lb/>&amp; <lb/>as describing the agent&apos;s beliefs after making the sequence <lb/>± <lb/>of <lb/>observations, starting from her initial epistemic state. Lehmann allows all possible sequences <lb/>of consistent formulas. Thus, he assumes that the agent does not observe false. We view <lb/>Lehmann&apos;s approach essentially as taking the agent&apos;s epistemic state to be the sequence of <lb/></body>

			<page>9 <lb/></page>

			<body>observations made, with the obvious revision operator that concatenate a new observation <lb/>to the current epistemic state. The properties of belief change depend on the function Bel. <lb/>Lehmann require Bel to satisfy the following postulates (where <lb/>± <lb/>and <lb/>² <lb/>denote sequences of <lb/>formulas, and <lb/>X <lb/>i s the concatenation operator): <lb/>I1. Bel <lb/>± <lb/>&amp; <lb/>i s a consistent belief set <lb/>I2. <lb/>¤ <lb/>P I <lb/>Bel <lb/>± <lb/>X <lb/>¤ <lb/>&amp; <lb/>I3. If <lb/>a <lb/>I <lb/>Bel <lb/>± <lb/>X <lb/>¤ <lb/>&amp; <lb/>, then <lb/>¤ <lb/>7 ³ <lb/>a <lb/>I <lb/>Bel <lb/>± <lb/>&amp; <lb/>I4. If <lb/>¤ <lb/>R I <lb/>Bel <lb/>± <lb/>&amp; <lb/>, then Bel <lb/>± <lb/>4 X <lb/>¤ <lb/>X <lb/>² <lb/>¤ &amp; <lb/>Bel <lb/>± <lb/>4 X <lb/>² <lb/>¤ &amp; <lb/>I5. If <lb/>a <lb/>D u <lb/>h ¤ <lb/>, then Bel <lb/>± <lb/>4 X <lb/>¤ <lb/>X <lb/>b a <lb/>R X <lb/>b ² <lb/>¤ &amp; <lb/>Bel <lb/>± <lb/>X <lb/>b a <lb/>7 X <lb/>² <lb/>¤ &amp; <lb/>I6. If <lb/>0 <lb/>t a <lb/>E I <lb/>Bel <lb/>± <lb/>4 X <lb/>¤ <lb/>&amp; <lb/>, then Bel <lb/>± <lb/>X <lb/>¤ <lb/>X <lb/>b a <lb/>µ X <lb/>² <lb/>¤ &amp; <lb/>Bel <lb/>± <lb/> ¶ X <lb/>y ¤ <lb/>X <lb/>¤ <lb/># <lb/>a <lb/>µ X <lb/>² <lb/>¤ &amp; <lb/>I7. Bel <lb/>± <lb/>X <lb/>£ 0 <lb/>6 ¤ <lb/>X <lb/>¤ <lb/>&amp; <lb/>d y <lb/>Cl Bel <lb/>± <lb/>&amp; <lb/>s <lb/>e 8 <lb/>F ¤ <lb/>@ <lb/>&amp; <lb/>We refer the interested reader to [Leh95] for the motivation for these postulates. As Lehmann <lb/>notes, the spirit of the original AGM postulates is captured by these postulates. Lehmann views <lb/>I5 and I7 as two main additions to the basic AGM postulates. He states that &quot;Since postulates <lb/>I5 and I7 seem secure, i.e., difficult to reject, the postulates I1-I7 may probably be considered <lb/>as a reasonable formalization of the intuitions of AGM.&quot; Our view is that it is impossible to <lb/>decide whether to accept or reject postulates such as I5 or I7 (or, for that matter, any of the <lb/>other postulates) without an explicit ontology. There may be ontologies for which I5 and I7 <lb/>are reasonable, and others for which they are not. &quot;Reasonableness&quot; is not an independently <lb/>defined notion; it depends on the ontology. The ontology of the next section emphasizes this <lb/>point. <lb/>4 Taking Observations to be Knowledge <lb/>We now consider an ontology where observations are taken to be knowledge. In this ontology, <lb/>it is impossible to observe false. In fact, it is impossible to make any inconsistent sequence of <lb/>observations. That is, if <lb/>¤ <lb/>1 <lb/>x <lb/>r <lb/>b r <lb/>b r <lb/>x <lb/>¤ <lb/>s z <lb/>i s observed, then <lb/>¤ <lb/>1 <lb/># <lb/>r <lb/>b r <lb/>y r# <lb/>( ¤ <lb/>s z <lb/>m ust be consistent (although <lb/>it may not be consistent with the agent&apos;s original beliefs). <lb/>In earlier work [FH95b], we presented such an ontology, based on Halpern and Fagin&apos;s <lb/>[HF89] framework of multi-agent systems (see [FHMV95] for more details). The key assump-<lb/>tion in the multi-agent system framework is that we can characterize the system by describing <lb/>it in terms of a state that changes over time. Formally, we assume that at each point in time, the <lb/>agent is in some local state. Intuitively, this local state encodes the information the agent has <lb/>observed thus far. There is also an environment, whose state encodes relevant aspects of the <lb/>system that are not part of the agent&apos;s local state. A global state is a tuple <lb/>• <lb/>b <lb/>x <lb/>• <lb/>¹ <lb/>b &amp; <lb/>c onsisting of <lb/>the environment state <lb/>• <lb/>b <lb/>a nd the local state <lb/>• <lb/>b ¹ <lb/>o f the agent. A run of the system is a function <lb/>from time (which, for ease of exposition, we assume ranges over the natural numbers) to global <lb/>states. Thus, if <lb/>º <lb/>is a run, then <lb/>º <lb/>0&amp; <lb/>x <lb/>º <lb/>&apos; <lb/>1&amp; <lb/>x <lb/>r <lb/>y r <lb/>b r <lb/>i s a sequence of global states that, roughly <lb/>speaking, is a complete description of what happens over time in one possible execution of the <lb/>system. We take a system to consist of a set of runs. Intuitively, these runs describe all the <lb/></body>

			<page>10 <lb/></page>

			<body>possible behaviors of the system, that is, all the possible sequences of events that could occur <lb/>in the system over time. <lb/>Given a system <lb/>» <lb/>, we refer to a pair <lb/>q º <lb/>x <lb/>¼ <lb/>&amp; <lb/>c onsisting of a run <lb/>º <lb/>I <lb/>7 » <lb/>and a time <lb/>¼ <lb/>as a <lb/>point. If <lb/>º <lb/>¼ <lb/>&amp; <lb/>C <lb/>½ <lb/>• <lb/>b <lb/>x <lb/>• <lb/>b ¹ <lb/>f &amp; <lb/>, we define <lb/>º <lb/>y ¹ <lb/>b <lb/>¼ <lb/>&amp; <lb/>w <lb/>¾ • <lb/>b ¹ <lb/>and <lb/>º <lb/>b <lb/>Y <lb/>¼ <lb/>&amp; <lb/>t • <lb/>b <lb/>. We say two points <lb/>º <lb/>x <lb/>¼ <lb/>&amp; <lb/>and <lb/>ºf <lb/>x <lb/>¼ <lb/>f&amp; <lb/>a re indistinguishable to the agent, and write <lb/>q º <lb/>x <lb/>¼ <lb/>&amp; <lb/>¿ <lb/>À ¹ <lb/>q ºf <lb/>x <lb/>p ¼ <lb/>f&amp; <lb/>, if <lb/>º <lb/>b ¹ <lb/>¼ <lb/>&amp; <lb/>ºf <lb/>¹ <lb/>¼ <lb/>f&amp; <lb/>, <lb/>i.e., if the agent has the same local state at both points. Finally, an interpreted system is a tuple <lb/>q » <lb/>x <lb/>Á <lb/>&amp; <lb/>, consisting of a system <lb/>» <lb/>together with a mapping <lb/>Á <lb/>that associates with each point a <lb/>truth assignment to the primitive propositions. <lb/>To capture the AGM framework, we consider a special class of interpreted systems: We <lb/>fix a propositional <lb/>£ <lb/>. We assume that the agent makes observations, which are characterized <lb/>by formulas in <lb/>£ <lb/>, and that her local state consists of the sequence of observations that she has <lb/>made. We assume that the environment&apos;s local state describes which formulas are actually true <lb/>in the world, so that it is a truth assignment to the formulas in <lb/>£ <lb/>. As observed by Katsuno and <lb/>Mendelzon [KM91a], the AGM postulates assume that the world is static; to capture this, we <lb/>assume that the environment state does not change over time. Formally, we are interested in <lb/>the unique interpreted system <lb/>q » <lb/>4 Â <lb/>Ã <lb/>¥ Ä <lb/>x <lb/>Á <lb/>&amp; <lb/>t hat consists of all runs satisfying the following two <lb/>assumptions for every point <lb/>q º <lb/>x <lb/>¼ <lb/>&amp; <lb/>: <lb/>The environment&apos;s state <lb/>º¸ <lb/>¼ <lb/>&amp; <lb/>i s a truth assignment to the formulas in <lb/>£ <lb/>that agrees with <lb/>Á <lb/>at <lb/>º <lb/>x <lb/>¼ <lb/>&amp; <lb/>( that is, <lb/>Á <lb/>º <lb/>x <lb/>p ¼ <lb/>&amp; <lb/>t <lb/>º <lb/>b <lb/>9 <lb/>¼ <lb/>&amp; <lb/>) , and <lb/>º <lb/>y <lb/>Å <lb/>¼ <lb/>&amp; <lb/>º <lb/>b <lb/>Y <lb/>0&amp; . <lb/>The agent&apos;s state <lb/>º <lb/>b ¹ <lb/>¼ <lb/>&amp; <lb/>i s a sequence of the form <lb/>AE <lb/>! ¤ <lb/>1 <lb/>x <lb/>r <lb/>y r <lb/>b r <lb/>x <lb/>¤ <lb/>| Ç <lb/>w È <lb/>, such that <lb/>¤ <lb/>1 <lb/># <lb/>r <lb/>b r <lb/>y r# <lb/>¤ <lb/>s Ç <lb/> is <lb/>true according to the truth assignment <lb/>º <lb/>b <lb/>Y <lb/>¼ <lb/>&amp; <lb/> a nd <lb/>º <lb/>1 <lb/>¼ <lb/>1&amp; <lb/>¾ AE <lb/>¤ <lb/>1 <lb/>x <lb/>r <lb/>b r <lb/>y r <lb/>x <lb/>¤ <lb/>| Ç <lb/>d É <lb/>1 <lb/>È <lb/>. <lb/>Notice that the form of the agent&apos;s state makes explicit an important implicit assumption: that <lb/>the agent remembers all her previous observations. <lb/>In an interpreted system, we can talk about an agent&apos;s knowledge: the agent knows <lb/>¤ <lb/>at <lb/>a point <lb/>q º <lb/>x <lb/>¼ <lb/>&amp; <lb/>i f <lb/>¤ <lb/>holds in all points <lb/>º <lb/>f f <lb/>x <lb/>¼ <lb/>f <lb/>&amp; <lb/>s uch that <lb/>q º <lb/>x <lb/>¼ <lb/>&amp; <lb/>¿ <lb/>¹ <lb/>q º <lb/>f <lb/>x <lb/>¼ <lb/>f <lb/>Ê &amp; <lb/>. It is easy to see <lb/>that, according to this definition, if <lb/>º¹ <lb/>¼ <lb/>&amp; <lb/>U AE <lb/>¤ <lb/>1 <lb/>x <lb/>r <lb/>b r <lb/>y r <lb/>x <lb/>¤Ç <lb/>È <lb/>, then the agent knows <lb/>¤ <lb/>1 <lb/># <lb/>r <lb/>b r <lb/>y r# <lb/>¤Ç <lb/>at the point <lb/>q º <lb/>x <lb/>¼ <lb/>&amp; <lb/>: the agent&apos;s observations are known to be true in this approach. We are <lb/>interested in talking about the agent&apos;s beliefs as well as her knowledge. To allow this, we <lb/>added a notion of plausibility to interpreted systems in [FH95a]. We consider a variant of this <lb/>approach here, using OCFs, since it makes it easier to relate our observations to Darwiche and <lb/>Pearl&apos;s framework. <lb/>We assume that we start with an OCF <lb/>o n runs such that <lb/>w <lb/>º <lb/>&apos; &amp; <lb/>À <lb/>for any run <lb/>º <lb/>. Intuitively, <lb/>represents our prior ranking on runs. Initially, no runs is viewed as impossible. We then <lb/>associate, with each point <lb/>q º <lb/>x <lb/>¼ <lb/>&amp; <lb/>, an OCF <lb/>« <lb/>Ì Ë¡ <lb/>Ç <lb/>Í ® <lb/>on the runs. We define <lb/>« <lb/>Ì Ë¡ <lb/>Ç <lb/>Í ® <lb/>by induction on <lb/>¼ <lb/>. We take <lb/>« <lb/>Î Ë¡ <lb/>0 <lb/>® <lb/>E <lb/>, and we take <lb/>« <lb/>Î Ë¡ <lb/>Ç <lb/>} <lb/>1 <lb/>® <lb/>E <lb/>« <lb/>Ì Ë¡ <lb/>Ç <lb/>Í ® <lb/>b Ï <lb/>w Ð <lb/>1 <lb/>¡ <lb/>Ñ <lb/>, where <lb/>º <lb/>b ¹ <lb/>b <lb/>1 <lb/>1&amp; <lb/>d <lb/>½ AE <lb/>¤ <lb/>1 <lb/>x <lb/>r <lb/>b r <lb/>y r <lb/>x <lb/>¤ <lb/>s Ç <lb/>} <lb/>1 <lb/>È <lb/>. <lb/>Thus, <lb/>« <lb/>Ì Ë¡ <lb/>Ç <lb/>Í } <lb/>1 <lb/>® <lb/>is the result of conditioning <lb/>« <lb/>Î Ë¡ <lb/>Ç <lb/>® <lb/>on the last observation the agent made, <lb/>giving it degree of firmness <lb/>. Thus, the agent is treating the observations as knowledge in <lb/>a manner compatible with the semantics for knowledge in the interpreted system. Moreover, <lb/>since obervations are known, they are also believed. <lb/>As we show in the full paper, this framework satisfies the AGM postulates R1f -R8f , inter-<lb/>preted on epistemic states. (Here we take the agent&apos;s epistemic state at the point <lb/>º <lb/>x <lb/>¼ <lb/>&amp; <lb/>t o consist <lb/>of <lb/>º <lb/>b ¹ <lb/>b <lb/>¼ <lb/>&amp; <lb/>t ogether with <lb/>« <lb/>Ì Ë¡ <lb/>Ç <lb/>Í ® <lb/>.) Moreover, the framework also satisfies Darwiche and Pearl&apos;s <lb/></body>

			<page>11 <lb/></page>

			<body>postulates (appropriately modified to apply to epistemic states), except that the contentious C2 <lb/>is now vacuous, since it is illegal to revise by <lb/>a <lb/>and then <lb/>¤ <lb/>if <lb/>¤ <lb/>5 u <lb/>0 <lb/>a <lb/>. <lb/>How does this framework compare to Lehmann&apos;s? Like Lehmann&apos;s, there is an explicit <lb/>attempt to associate beliefs with a sequence of revisions. However, we have restricted the <lb/>sequence of revisions, since we are treating observations as knowledge. It is easy to see that <lb/>I1-I3 and I5-I7 hold in our framework. However, since we have restricted the sequence of <lb/>observations allowed, some of these postulates are much weaker in our framework than in <lb/>Lehmann&apos;s. In particular, I7 is satisfied vacuously, since we do not allow a sequence of the <lb/>form <lb/>± <lb/>X <lb/>£ 0 <lb/>6 ¤ <lb/>X <lb/>¤ <lb/>. On the other hand, I4 is not satisfied in our framework. Our discussion in the <lb/>introduction suggests a counterexample. Suppose that initially, <lb/>w <lb/>&quot; © <lb/># <lb/>e <lb/>&apos; &amp; <lb/>d <lb/>0, <lb/>w <lb/>0 <lb/>2 © <lb/>g # <lb/>e <lb/>&amp; <lb/>1, <lb/>w <lb/>&quot; © <lb/># <lb/>7 0 <lb/>6 <lb/>&apos; &amp; <lb/>2, and <lb/>A <lb/>p 0 <lb/>2 © <lb/> ¶ # <lb/>7 0 <lb/>6 <lb/>&apos; &amp; <lb/>3. Thus, initially the agent believes both <lb/>© <lb/>and <lb/>, but <lb/>believes <lb/>© <lb/>with firmness 1 and <lb/>with firmness 2. If the agent then observes <lb/>0 <lb/>2 © <lb/>g 3 <lb/>0 <lb/>6 <lb/>, he will <lb/>then believe <lb/>b <lb/>ut not <lb/>© <lb/>. On the other hand, suppose the agent first observes <lb/>© <lb/>. He still believes <lb/>both <lb/>© <lb/>and <lb/>, of course, but now <lb/>© <lb/>is believed with firmness <lb/>. That means if he then observes <lb/>0 <lb/>2 © <lb/>q 3 <lb/>0 <lb/>6 <lb/>, he will believe <lb/>, but not <lb/>© <lb/>, violating I4. However, a weaker variant of I4 does hold <lb/>in our system: if the agent knows <lb/> ¤ <lb/>, then observing <lb/>¤ <lb/>will not change her future beliefs. <lb/>5 Discussion <lb/>The goal of this paper was to highlight what we see as some methodological problems in much <lb/>of the literature on belief revision. There has been (in our opinion) too much attention paid <lb/>to postulates, and not enough to the underlying ontology. An ontology must make clear what <lb/>the agent&apos;s epistemic state is, what types of observations the agent can make, the status of <lb/>observations, and how the agent goes about revising the epistemic state. Previous work has <lb/>typically not made clear whether observations are believed to be true or known to be true, and <lb/>if they are believed, what the strength of belief is. This issue is particularly important if we <lb/>have epistemic states like rankings that are richer than belief sets. If observations are believed, <lb/>but not necessarily known, to be true, then it is not clear how to go about revising such a richer <lb/>epistemic state. With what degree of firmness should the new belief be held? No particular <lb/>answer seems to us that well motivated. It may be appropriate for the user to attach degrees of <lb/>firmness to observations, as was done in [Gol92, Wil94, Wob95] (following the lead of Spohn <lb/>[Spo88]); we can even generalize to allowing uncertain observations [DP92]. <lb/>It seems to us that many of the intuitions that researchers in the area have are motivated by <lb/>thinking in terms of observations as known, even if this is not always reflected in the postulates <lb/>considered. We have examined carefully one particular instantiation of this ontology, that of <lb/>treating observations as knowledge. (As shown in [FH95b], this ontology can also capture <lb/>Katsuno and Mendelzon&apos;s belief update [KM91b].) We have shown that, in this ontology, some <lb/>postulates that seem reasonable, such as Lehmann&apos;s I4, do not hold. We do not mean to suggest <lb/>that I4 is &quot;wrong&quot; (whatever that might mean in this context). Rather, it shows that we cannot <lb/>blithely accept postulates without making the underlying ontology clear. We would encourage <lb/>the investigation of other ontologies for belief change. <lb/></body>

			<page>12 <lb/></page>

			<listBibl>References <lb/>[AGM85] C. E. Alchourrón, P. Gärdenfors, and D. Makinson. On the logic of theory change: <lb/>partial meet functions for contraction and revision. Journal of Symbolic Logic, <lb/>50:510-530, 1985. <lb/>[BG93] <lb/>C. Boutilier and M. Goldszmidt. Revising by conditional beliefs. In Proc. National <lb/>Conference on Artificial Intelligence (AAAI &apos;93), pages 648-654. 1993. <lb/>[Bou92] <lb/>C. Boutilier. Normative, subjective and autoepistemic defaults: adopting the <lb/>Ramsey test. In KR &apos;92, pages 685-696. <lb/>[Bou93] <lb/>C. Boutilier. Revision sequences and nested conditionals. In Proc. Thirteenth <lb/>International Joint Conference on Artificial Intelligence (IJCAI &apos;93), pages 519-<lb/>525, 1993. <lb/>[Bou94] <lb/>C. Boutilier. Unifying default reasoning and belief revision in a modal framework. <lb/>Artificial Intelligence, 68:33-85, 1994. <lb/>[DP92] <lb/>D. Dubois and H. Prade. Belief change and possibility theory. In P. Gärdenfors, <lb/>editor, Belief Revision. Cambridge University Press, Cambridge, U.K., 1992. <lb/>[DP94] <lb/>A. Darwiche and J. Pearl. On the logic of iterated belief revision. In Theoretical <lb/>Aspects of Reasoning about Knowledge: Proc. Fifth Conference, pages 5-23. <lb/>1994. <lb/>[FH95a] <lb/>N. Friedman and J. Y. Halpern. Modeling belief in dynamic systems. part I: <lb/>foundations. Technical Report RJ 9965, IBM, 1995. Submitted for publication. A <lb/>preliminary version appears in R. Fagin editor. Theoretical Aspects of Reasoning <lb/>about Knowledge: Proc. Fifth Conference, 1994, pp. 44-64, under the title &quot;A <lb/>knowledge-based framework for belief change. Part I: foundations&quot;. <lb/>[FH95b] <lb/>N. Friedman and J. Y. Halpern. Modeling belief in dynamic systems. Part II: <lb/>revision and update. In preparation. A preliminary version appears in J. Doyle, <lb/>E. Sandewall, and P. Torasso, editors. Principles of Knowledge Representation and <lb/>Reasoning: Proc. Fourth International Conference (KR &apos;94), 1994, pp. 190-201, <lb/>under the title &quot;A knowledge-based framework for belief change. Part II: revision <lb/>and update.&quot;, 1995. <lb/>[FHMV95] R. Fagin, J. Y. Halpern, Y. Moses, and M. Y. Vardi. Reasoning about Knowledge. <lb/>MIT Press, Cambridge, Mass., 1995. <lb/>[FL94] <lb/>M. Freund and D. Lehmann. Belief revision and rational inference. Technical <lb/>Report TR 94-16, Hebrew University, 1994. <lb/>[Gär88] <lb/>P. Gärdenfors. Knowledge in Flux. MIT Press, Cambridge, U.K., 1988. <lb/>[GM88] <lb/>P. Gärdenfors and D. Makinson. Revisions of knowledge systems using epistemic <lb/>entrenchment. In Proc. Second Conference on Theoretical Aspects of Reasoning <lb/>about Knowledge, pages 83-95. 1988. <lb/></listBibl>

			<page>13 <lb/></page>

			<listBibl>[Gol92] <lb/>M. Goldszmidt. Qualitative probabilities: a normative framework for common-<lb/>sense reasoning. PhD thesis, University of California Los Angeles, 1992. <lb/>[GP92] <lb/>M. Goldszmidt and J. Pearl. Rank-based systems: A simple approach to belief <lb/>revision, belief update and reasoning about evidence and actions. In KR &apos;92, pages <lb/>661-672. <lb/>[Gro88] <lb/>A. Grove. Two modelings for theory change. Journal of Philosophical Logic, <lb/>17:157-170, 1988. <lb/>[HF89] <lb/>J. Y. Halpern and R. Fagin. Modelling knowledge and action in distributed systems. <lb/>Distributed Computing, 3(4):159-179, 1989. A preliminary version appeared in <lb/>Proc. 4th ACM Symposium on Principles of Distributed Computing, 1985, with <lb/>the title &quot;A formal model of knowledge, action, and communication in distributed <lb/>systems: preliminary report&quot;. <lb/>[KM91a] H. Katsuno and A. Mendelzon. On the difference between updating a knowledge <lb/>base and revising it. In KR &apos;91, pages 387-394. 1991. <lb/>[KM91b] H. Katsuno and A. Mendelzon. Propositional knowledge base revision and minimal <lb/>change. Artificial Intelligence, 52(3):263-294, 1991. <lb/>[Leh95] <lb/>D. Lehmann. Belief revision, revised. In Proc. Fourteenth International Joint <lb/>Conference on Artificial Intelligence (IJCAI &apos;95), pages 1534-1540. 1995. <lb/>[Lev88] <lb/>I. Levi. Iteration of conditionals and the Ramsey test. Synthese, 76:49-81, 1988. <lb/>[Rot89] <lb/>H. Rott. Conditionals and theory change: revision, expansions, and additions. <lb/>Synthese, 81:91-113, 1989. <lb/>[Sha76] <lb/>G. Shafer. A Mathematical Theory of Evidence. Princeton University Press, <lb/>Princeton, N.J., 1976. <lb/>[Spo88] <lb/>W. Spohn. Ordinal conditional functions: a dynamic theory of epistemic states. <lb/>In W. Harper and B. Skyrms, editors, Causation in Decision, Belief Change and <lb/>Statistics, volume 2, pages 105-134. Reidel, Dordrecht, Netherlands, 1988. <lb/>[Wil94] <lb/>M. Williams. Transmutations of knowledge systems. In KR &apos;94, pages 619-629. <lb/>1994. <lb/>[Wob95] W. Wobcke. Belief revision, conditional logic, and nonmonotonic reasoning. Notre <lb/>Dame Journal of Formal Logic, 36(1):55-102, 1995. <lb/></listBibl>

			<page>14 </page>


	</text>
</tei>
